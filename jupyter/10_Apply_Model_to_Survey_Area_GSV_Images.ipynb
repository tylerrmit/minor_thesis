{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d64d1c3",
   "metadata": {},
   "source": [
    "# 10. Apply Model to Survey Area GSV Images\n",
    "\n",
    "For a given locality, and OpenStreetMap XML extracts for it, find all intersections, and create a CSV batch file listing all the points we will sample.  After checking the scope -- how many images will be downloaded? -- go ahead and download any GSV images not already cached.  Then apply the model to detect bicycle lanes, and record detections in a \"detection_log\" CSV file.  Then, match the detections to the OpenStreetMap data, and create geojson files to compare the data and draw routes on a map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69cf8c",
   "metadata": {},
   "source": [
    "We walk down each \"way\" in the OSM data and work out a heading  at each point, based on the average of the bearing from the previous point, and the bearing to the next point, in order to have some idea which heading to use in a Google Street View request to sample images (roughly) forward/backward/left/right.\n",
    "\n",
    "The items we are looking for might not be most visible from right in the middle of an intersection, therefore there is an option to specify a range around each intersection that we want to sample, along the heading.  E.g. if we specify 20m, then we will sample the point in the middle of the intersection, +/- 10m, and +/- 20m.  We use 10m intervals within this range because Google Street View typically gives a different image roughly every 10m.\n",
    "\n",
    "In OSM, a long street may be divided up into multiple connecting \"ways\", each with the same name.  Rather than walking down ways in random order, we attempt process ways in order of their name, and then within the name we attempt to identify a logical order:  Start with a way whose first node is NOT an intersection, then find the next way with the same name that intersects with the end of the first, and so on.  This isn't really necessary to generate a map of all the detections, but it is useful when visually inspecting the results to assess or measure the quality of the results.  It is less disorientating for a human to see the images in a logical \"walking\" order.\n",
    "\n",
    "Once we have a list of points to sample, we output a batch \"csv\" containing the way id, the node id of the sample point, the offset in metres (e.g. \"-20\"), the latitude, longitude, and bearing.  This can then be used to download and cache Google Street View images, and process them with our detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24497380",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Any configuration that is required to run this notebook can be customized in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee8c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which \"locality\" do we wish to process?\n",
    "# Assumes that we can find a pair of OSM files with corresponding names,\n",
    "# extracted with the \"osmium\" tool.  One file follows the official shape of\n",
    "# the locality, while a second file follows a bounding box around the locality\n",
    "# with a 200m margin, so that when we are looking for intersections, we don't\n",
    "# miss any due to the intersecting road being just outside the boundary of the\n",
    "# locality (apart from the intersection).\n",
    "#locality = 'Mount Eliza'\n",
    "locality = 'Mount Eliza Sample'\n",
    "#locality = 'Heidelberg Sample'\n",
    "\n",
    "# The locality 'Mount Eliza' was extracted from OpenStreetMap according to the\n",
    "# official geographic boundary.  However, sampling the entire suburb within\n",
    "# 20 metres of every intersection would yield 7,049 sample locations.  Each\n",
    "# sample location requires 4 images (front, left, right, rear) giving a total\n",
    "# of 28,196 images required from the Google Street View API, at an approximate cost\n",
    "# of $197 USD.  Therefore, a smaller region \"Mount Eliza Sample\" was extracted\n",
    "# from OpenStreetMap using the \"osmium extract -bbox\" option as follows:\n",
    "\n",
    "# osmium extract --bbox=145.094,-38.176,145.110,-38.162 australia-latest.osm.pbf -o Locality_Mount_Eliza_Sample.osm\n",
    "# osmium extract --bbox=145.092,-38.178,145.112,-38.160 australia-latest.osm.pbf -o Locality_Mount_Eliza_Sample_margin.osm\n",
    "\n",
    "# This sample region contains a mix of roads with and without bicycle lanes,\n",
    "# a regional highway, and small no-through roads, and is intended to be\n",
    "# representative of the region, without a high API cost.  It yields 1,113\n",
    "# sample locations, requiring 4,452 images from the Google Street View API, at\n",
    "# an approximate cost of $31 USD.\n",
    "\n",
    "# We will sample the middle of each intersection, but we can also sample a\n",
    "# \"margin\" around the intersection, at 10m intervals.\n",
    "# E.g. if we set this to \"20\" then we will sample points at:\n",
    "#    -20m, -10m, 0m, 10m, and 20m\n",
    "# from the centre of the intersection, along the assumed bearing of the road\n",
    "margin = 20\n",
    "\n",
    "# Trained detection model name\n",
    "trained_model_name   = 'faster_rcnn_V1_2000'\n",
    "\n",
    "# Prefix that will be included as a suffix in the label map file and tfrecord train and test files\n",
    "dataset_version = 'V1'\n",
    "\n",
    "# Confidence threshold to apply with the model.  The model must be at least\n",
    "# this confident of a detection for it to count\n",
    "confidence_threshold = 0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c357d8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce231461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure local modules can be imported\n",
    "module_path_root = os.path.abspath(os.pardir)\n",
    "if module_path_root not in sys.path:\n",
    "    sys.path.append(module_path_root)\n",
    "    \n",
    "# Import local modules\n",
    "import osm_gsv_utils.osm_walker as osm_walker\n",
    "import osm_gsv_utils.gsv_loader as gsv_loader\n",
    "import tf2_utils.tf2_model_wrapper as tf2_model_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa9e44",
   "metadata": {},
   "source": [
    "## Identify sample points\n",
    "\n",
    "Load the OSM data, and then generate lists of sample points at margins of 0, +/- 10m, and +/- 20m from each intersection,\n",
    "and report on how many samples are found for each sample setting, to get an idea of the impact of increasing/decreasing\n",
    "the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ec609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive paths for configuration\n",
    "\n",
    "# A version of the locality name with spaces replaced by underscores\n",
    "locality_clean   = locality.replace(' ', '_')\n",
    "\n",
    "# Name of the locality with the margin around intersections included\n",
    "locality_margin  = '{0:s}_{1:d}m'.format(locality_clean, margin)\n",
    "\n",
    "# Paths to both the main OpenStreetMap XML extract and a second extract that allows a wider margin\n",
    "filename_main    = os.path.join(os.path.abspath(os.pardir), 'data_sources', 'Locality_' + locality_clean + '.osm')\n",
    "filename_margin  = os.path.join(os.path.abspath(os.pardir), 'data_sources', 'Locality_' + locality_clean + '_margin.osm')\n",
    "\n",
    "# Batch file where the list of points to sample from GSV is written\n",
    "batch_filename   = os.path.join(module_path_root, 'batches', locality_margin + '.csv')\n",
    "\n",
    "# Derived GSV download/cache directory\n",
    "gsv_download_dir = os.path.join(os.path.abspath(os.pardir), 'data_sources', 'gsv')\n",
    "\n",
    "# Filename containing API key for connecting to Google Street View\n",
    "apikey_filename    = os.path.join(module_path_root, 'apikey.txt')\n",
    "\n",
    "# Output directory where the detection log and any images with detection overlays will be written\n",
    "output_directory = os.path.join(module_path_root, 'detections', locality_margin)\n",
    "\n",
    "# Output CSV file with a log of all detections\n",
    "detection_log_path = os.path.join(output_directory, 'detection_log.csv')\n",
    "\n",
    "# Change directory to make sure the detection model dependencies are found\n",
    "os.chdir(Path(module_path_root).parent.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab209061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e55702da245e7b2fbb71b4079620c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2e607db4b64eb985b92d3243469431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+/- 20m: 1113\n",
      "+/- 10m: 705\n",
      "+/- 00m: 297\n"
     ]
    }
   ],
   "source": [
    "# Load OSM data into memory\n",
    "# The \"main\" file is the exact area we want to cover\n",
    "# The \"margin\" file is a slightly larger extract to capture any intersections\n",
    "# at the margin of the main file.  There may be roads JUST outside the area\n",
    "# being surveyed that only just touch the survey area at an intersection.\n",
    "# If these roads are clipped from the main file, then they wouldn't show up\n",
    "# as roads that share a common node, and therefore the intersections would\n",
    "# not be detected.\n",
    "walker = osm_walker(filename_main, filename_margin, verbose=False)\n",
    "\n",
    "# Generate sample lists with different margin settings, and report sample point count for each\n",
    "sample_points_20 = walker.sample_all_way_intersections(-20, +20, 10, ordered=True, verbose=False)\n",
    "sample_points_10 = walker.sample_all_way_intersections(-10, +10, 10, ordered=True, verbose=False)\n",
    "sample_points_00 = walker.sample_all_way_intersections(  0,   0, 10, ordered=True, verbose=False)\n",
    "\n",
    "# How many GSV locations would we be downloading for each of these options?\n",
    "print('+/- 20m: ' + str(len(sample_points_20)))\n",
    "print('+/- 10m: ' + str(len(sample_points_10)))\n",
    "print('+/- 00m: ' + str(len(sample_points_00)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4670c",
   "metadata": {},
   "source": [
    "## Download/Cache GSV images\n",
    "\n",
    "Download GSV images (if not already cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c234468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup E:\\Release\\minor_thesis\\batches\\Mount_Eliza_Sample_20m.csv to E:\\Release\\minor_thesis\\batches\\Mount_Eliza_Sample_20m20211012_013239.csv\n",
      "Write E:\\Release\\minor_thesis\\batches\\Mount_Eliza_Sample_20m.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4d709624c94c1dbd804a57b83aa24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSV Cache Hits:       4452 Misses:          0\n"
     ]
    }
   ],
   "source": [
    "# Initialise interface to Google Street View\n",
    "gsv = gsv_loader(apikey_filename, gsv_download_dir)\n",
    "\n",
    "# Create a batch file (CSV) with the list of locations to download from Google\n",
    "# limit=0 means unlimited, set to a small integer to test a few downloads\n",
    "gsv.write_batch_file(batch_filename, sample_points_20, limit=0)\n",
    "\n",
    "# Process the batch file (with progress bar) and report how many images were fetched vs. skipped\n",
    "# Working from a batch file means we have a permanent record of what was loaded (in case we need to resume later)\n",
    "# and helps us implement a progress bar via tdqm\n",
    "gsv.process_batch_file(batch_filename, progress=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428fe3a",
   "metadata": {},
   "source": [
    "## Apply Model to Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1702b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Output directory for detections: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\n",
      "Label Map Path: [TensorFlow\\workspace\\annotations\\label_map_V1.pbtxt]\n",
      "Latest Checkpoint: ckpt-0\n"
     ]
    }
   ],
   "source": [
    "# Initialise model\n",
    "model_wrapper = tf2_model_wrapper(\n",
    "    locality, \n",
    "    margin, \n",
    "    gsv_download_dir, \n",
    "    output_directory, \n",
    "    trained_model_name, \n",
    "    version_suffix = dataset_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d49fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d33b96ce7f5496a89b7c9b22d6ffc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From C:\\Users\\User\\TensorFlow\\models\\research\\object_detection\\utils\\model_util.py:57: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:\\\\Release\\\\minor_thesis\\\\detections\\\\Mount_Eliza_Sample_20m\\\\detection_log.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run detections for entire batch\n",
    "model_wrapper.process_batch_file(batch_filename, min_score=confidence_threshold, progress=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9cf99",
   "metadata": {},
   "source": [
    "## Load Detections\n",
    "\n",
    "Load the detection log that was just created, correlate to the Open StreetMap data, and write geojson files to compare and draw the routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b7f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate detections to OpenStreetMap data\n",
    "walker.load_detection_log(detection_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f977d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hit, feature count: 21\n",
      "Writing to: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\\hit.geojson\n",
      "Writing tag, feature count: 2\n",
      "Writing to: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\\tag.geojson\n",
      "Writing both, feature count: 2\n",
      "Writing to: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\\both.geojson\n",
      "Writing either, feature count: 21\n",
      "Writing to: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\\either.geojson\n",
      "Writing hit_only, feature count: 20\n",
      "Writing to: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\\hit_only.geojson\n",
      "Writing tag_only, feature count: 0\n",
      "Writing to: E:\\Release\\minor_thesis\\detections\\Mount_Eliza_Sample_20m\\tag_only.geojson\n",
      "hit     : Total distance   11817.05m\n",
      "tag     : Total distance    2344.05m\n",
      "both    : Total distance    2344.05m\n",
      "either  : Total distance   11817.05m\n",
      "hit_only: Total distance    9387.53m\n",
      "tag_only: Total distance       0.00m\n"
     ]
    }
   ],
   "source": [
    "walker.write_geojsons(locality_margin, output_directory, intersection_skip_limit=1, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
